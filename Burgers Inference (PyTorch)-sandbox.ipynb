{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute\n",
    "\n",
    "**Original Work**: *Maziar Raissi, Paris Perdikaris, and George Em Karniadakis*\n",
    "\n",
    "**Github Repo** : https://github.com/maziarraissi/PINNs\n",
    "\n",
    "**Link:** https://github.com/maziarraissi/PINNs/tree/master/appendix/continuous_time_identification%20(Burgers)\n",
    "\n",
    "@article{raissi2017physicsI,\n",
    "  title={Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations},\n",
    "  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n",
    "  journal={arXiv preprint arXiv:1711.10561},\n",
    "  year={2017}\n",
    "}\n",
    "\n",
    "@article{raissi2017physicsII,\n",
    "  title={Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations},\n",
    "  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n",
    "  journal={arXiv preprint arXiv:1711.10566},\n",
    "  year={2017}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../Utilities/')\n",
    "\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "from pyDOE import lhs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from plotting import newfig, savefig\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "import time\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics-informed Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the physics-guided neural network\n",
    "class PhysicsInformedNN():\n",
    "    def __init__(self, X_u, u, X_f, layers, lb, ub, nu):\n",
    "        \n",
    "        # boundary conditions\n",
    "        self.lb = torch.tensor(lb).float().to(device)\n",
    "        self.ub = torch.tensor(ub).float().to(device)\n",
    "        \n",
    "        # data\n",
    "        self.x_u = torch.tensor(X_u[:, 0:1], requires_grad=True).float().to(device)\n",
    "        self.t_u = torch.tensor(X_u[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.x_f = torch.tensor(X_f[:, 0:1], requires_grad=True).float().to(device)\n",
    "        self.t_f = torch.tensor(X_f[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.u = torch.tensor(u).float().to(device)\n",
    "        \n",
    "        self.layers = layers\n",
    "        self.nu = nu\n",
    "        \n",
    "        # deep neural networks\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        \n",
    "        # optimizers: using the same settings\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(), \n",
    "            lr=1.0, \n",
    "            max_iter=50000, \n",
    "            max_eval=50000, \n",
    "            history_size=50,\n",
    "            tolerance_grad=1e-5, \n",
    "            tolerance_change=1.0 * np.finfo(float).eps,\n",
    "            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n",
    "        )\n",
    "\n",
    "        self.iter = 0\n",
    "        \n",
    "    def net_u(self, x, t):  \n",
    "        u = self.dnn(torch.cat([x, t], dim=1))\n",
    "        return u\n",
    "    \n",
    "    def net_f(self, x, t):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
    "        u = self.net_u(x, t)\n",
    "        \n",
    "        u_t = torch.autograd.grad(\n",
    "            u, t, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_xx = torch.autograd.grad(\n",
    "            u_x, x, \n",
    "            grad_outputs=torch.ones_like(u_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_tt = torch.autograd.grad(\n",
    "            u_t, t, \n",
    "            grad_outputs=torch.ones_like(u_t),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_xt = torch.autograd.grad(\n",
    "            u_x, t, \n",
    "            grad_outputs=torch.ones_like(u_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        f = u_t + u * u_x - self.nu * u_xx\n",
    "        return f\n",
    "    \n",
    "    def loss_func(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        u_pred = self.net_u(self.x_u, self.t_u)\n",
    "        f_pred = self.net_f(self.x_f, self.t_f)\n",
    "        loss_u = torch.mean((self.u - u_pred) ** 2)\n",
    "        loss_f = torch.mean(f_pred ** 2)\n",
    "        \n",
    "        loss = loss_u + loss_f\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "        if self.iter % 100 == 0:\n",
    "            print(\n",
    "                'Iter %d, Loss: %.5e, Loss_u: %.5e, Loss_f: %.5e' % (self.iter, loss.item(), loss_u.item(), loss_f.item())\n",
    "            )\n",
    "        return loss\n",
    "    \n",
    "    def train(self):\n",
    "        self.dnn.train()\n",
    "                \n",
    "        # Backward and optimize\n",
    "        self.optimizer.step(self.loss_func)\n",
    "\n",
    "            \n",
    "    def predict(self, X):\n",
    "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        u = self.net_u(x, t)\n",
    "        f = self.net_f(x, t)\n",
    "        u = u.detach().cpu().numpy()\n",
    "        f = f.detach().cpu().numpy()\n",
    "        return u, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_u_train, u_train, X_f_train\n",
    "# np.shape(X_f_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 0.01/np.pi\n",
    "noise = 0.0        \n",
    "\n",
    "N_u = 100\n",
    "N_f = 10000\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "\n",
    "data = scipy.io.loadmat('data/burgers_shock.mat')\n",
    "\n",
    "#t and x are used for a meshgrid:\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "#what exactly is Exact?\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)    \n",
    "\n",
    "xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
    "uu1 = Exact[0:1,:].T\n",
    "xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
    "uu2 = Exact[:,0:1]\n",
    "xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
    "uu3 = Exact[:,-1:]\n",
    "\n",
    "X_u_train = np.vstack([xx1, xx2, xx3])\n",
    "X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
    "X_f_train = np.vstack((X_f_train, X_u_train))\n",
    "u_train = np.vstack([uu1, uu2, uu3])\n",
    "\n",
    "idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
    "X_u_train = X_u_train[idx, :]\n",
    "u_train = u_train[idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
    "np.shape(X_f_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(X_u_train[:,1]==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PhysicsInformedNN(X_u_train, u_train, X_f_train, layers, lb, ub, nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100, Loss: 8.48529e-02, Loss_u: 5.71724e-02, Loss_f: 2.76805e-02\n",
      "Iter 200, Loss: 6.37123e-02, Loss_u: 4.21022e-02, Loss_f: 2.16101e-02\n",
      "Iter 300, Loss: 3.54248e-02, Loss_u: 2.32548e-02, Loss_f: 1.21700e-02\n",
      "Iter 400, Loss: 2.19168e-02, Loss_u: 1.35494e-02, Loss_f: 8.36738e-03\n",
      "Iter 500, Loss: 1.37309e-02, Loss_u: 8.15359e-03, Loss_f: 5.57728e-03\n",
      "Iter 600, Loss: 1.04698e-02, Loss_u: 5.86502e-03, Loss_f: 4.60482e-03\n",
      "Iter 700, Loss: 7.82561e-03, Loss_u: 4.42597e-03, Loss_f: 3.39964e-03\n",
      "Iter 800, Loss: 6.14517e-03, Loss_u: 3.48093e-03, Loss_f: 2.66424e-03\n",
      "Iter 900, Loss: 5.02755e-03, Loss_u: 2.81611e-03, Loss_f: 2.21144e-03\n",
      "Iter 1000, Loss: 3.94207e-03, Loss_u: 2.16425e-03, Loss_f: 1.77782e-03\n",
      "Iter 1100, Loss: 3.37466e-03, Loss_u: 1.88612e-03, Loss_f: 1.48854e-03\n",
      "Iter 1200, Loss: 2.86923e-03, Loss_u: 1.51031e-03, Loss_f: 1.35892e-03\n",
      "Iter 1300, Loss: 2.55698e-03, Loss_u: 1.39801e-03, Loss_f: 1.15897e-03\n",
      "Iter 1400, Loss: 2.31155e-03, Loss_u: 1.31105e-03, Loss_f: 1.00050e-03\n",
      "Iter 1500, Loss: 1.84225e-03, Loss_u: 9.90883e-04, Loss_f: 8.51366e-04\n",
      "Iter 1600, Loss: 1.47695e-03, Loss_u: 7.11899e-04, Loss_f: 7.65051e-04\n",
      "Iter 1700, Loss: 1.21594e-03, Loss_u: 5.47056e-04, Loss_f: 6.68883e-04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c1f9e29e095b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_old\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_stps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                     \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0;31m# multiply by initial Hessian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "               \n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 1.93 GiB total capacity; 283.46 MiB already allocated; 34.50 MiB free; 284.00 MiB reserved in total by PyTorch)\nException raised from malloc at /opt/conda/conda-bld/pytorch_1595629427478/work/c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x4d (0x7f6ad5af877d in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x20626 (0x7f6ad5d50626 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x214f4 (0x7f6ad5d514f4 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)\nframe #3: <unknown function> + 0x21b81 (0x7f6ad5d51b81 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)\nframe #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x249 (0x7f6ad8c52e39 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xd15c49 (0x7f6ad6c73c49 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0xd2fa77 (0x7f6ad6c8da77 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0xe450dd (0x7f6b0d0980dd in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #8: <unknown function> + 0xe453f7 (0x7f6b0d0983f7 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #9: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0xfa (0x7f6b0d1a2e7a in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: at::TensorIterator::fast_set_up(at::TensorIteratorConfig const&) + 0x56a (0x7f6b0ce303ea in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #11: at::TensorIterator::build(at::TensorIteratorConfig&) + 0x76 (0x7f6b0ce34456 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 0xdd (0x7f6b0ce34abd in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&, bool) + 0x14a (0x7f6b0ce34c6a in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: at::native::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x47 (0x7f6b0cb71b77 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: <unknown function> + 0xd001c0 (0x7f6ad6c5e1c0 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #16: <unknown function> + 0x7f66b4 (0x7f6b0ca496b4 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: at::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x183 (0x7f6b0d1799e3 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: <unknown function> + 0x2be54e1 (0x7f6b0ee384e1 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: <unknown function> + 0x7f66b4 (0x7f6b0ca496b4 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #20: at::Tensor::add(at::Tensor const&, c10::Scalar) const + 0x183 (0x7f6b0d2ed3d3 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #21: <unknown function> + 0x30dd790 (0x7f6b0f330790 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #22: <unknown function> + 0x30de4e3 (0x7f6b0f3314e3 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #23: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0xb80 (0x7f6b0f31efe0 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #24: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7f6b0f320401 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #25: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7f6b0f318579 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #26: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7f6b1364799a in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #27: <unknown function> + 0xc819d (0x7f6b4ad9c19d in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6)\nframe #28: <unknown function> + 0x9609 (0x7f6b4daae609 in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #29: clone + 0x43 (0x7f6b4d9d5293 in /lib/x86_64-linux-gnu/libc.so.6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e0abcf71ed69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mu_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0merror_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_star\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mu_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_star\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error u: %e'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merror_u\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c1f9e29e095b>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_u\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c1f9e29e095b>\u001b[0m in \u001b[0;36mnet_f\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         )[0]\n\u001b[1;32m     68\u001b[0m         u_xt = torch.autograd.grad(\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    190\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    191\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 1.93 GiB total capacity; 283.46 MiB already allocated; 34.50 MiB free; 284.00 MiB reserved in total by PyTorch)\nException raised from malloc at /opt/conda/conda-bld/pytorch_1595629427478/work/c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x4d (0x7f6ad5af877d in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x20626 (0x7f6ad5d50626 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x214f4 (0x7f6ad5d514f4 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)\nframe #3: <unknown function> + 0x21b81 (0x7f6ad5d51b81 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)\nframe #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x249 (0x7f6ad8c52e39 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xd15c49 (0x7f6ad6c73c49 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0xd2fa77 (0x7f6ad6c8da77 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0xe450dd (0x7f6b0d0980dd in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #8: <unknown function> + 0xe453f7 (0x7f6b0d0983f7 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #9: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0xfa (0x7f6b0d1a2e7a in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: at::TensorIterator::fast_set_up(at::TensorIteratorConfig const&) + 0x56a (0x7f6b0ce303ea in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #11: at::TensorIterator::build(at::TensorIteratorConfig&) + 0x76 (0x7f6b0ce34456 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 0xdd (0x7f6b0ce34abd in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&, bool) + 0x14a (0x7f6b0ce34c6a in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: at::native::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x47 (0x7f6b0cb71b77 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: <unknown function> + 0xd001c0 (0x7f6ad6c5e1c0 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #16: <unknown function> + 0x7f66b4 (0x7f6b0ca496b4 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: at::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x183 (0x7f6b0d1799e3 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: <unknown function> + 0x2be54e1 (0x7f6b0ee384e1 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: <unknown function> + 0x7f66b4 (0x7f6b0ca496b4 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #20: at::Tensor::add(at::Tensor const&, c10::Scalar) const + 0x183 (0x7f6b0d2ed3d3 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #21: <unknown function> + 0x30dd790 (0x7f6b0f330790 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #22: <unknown function> + 0x30de4e3 (0x7f6b0f3314e3 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #23: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0xb80 (0x7f6b0f31efe0 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #24: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7f6b0f320401 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #25: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7f6b0f318579 in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #26: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7f6b1364799a in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #27: <unknown function> + 0xc819d (0x7f6b4ad9c19d in /home/guy/anaconda3/envs/pytorch/lib/python3.7/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6)\nframe #28: <unknown function> + 0x9609 (0x7f6b4daae609 in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #29: clone + 0x43 (0x7f6b4d9d5293 in /lib/x86_64-linux-gnu/libc.so.6)\n"
     ]
    }
   ],
   "source": [
    "u_pred, f_pred = model.predict(X_star)\n",
    "\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "print('Error u: %e' % (error_u))                     \n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "Error = np.abs(Exact - U_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" The aesthetic setting has changed. \"\"\"\n",
    "\n",
    "####### Row 0: u(t,x) ##################    \n",
    "\n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
    "              extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "              origin='lower', aspect='auto')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.10)\n",
    "cbar = fig.colorbar(h, cax=cax)\n",
    "cbar.ax.tick_params(labelsize=15) \n",
    "\n",
    "ax.plot(\n",
    "    X_u_train[:,1], \n",
    "    X_u_train[:,0], \n",
    "    'kx', label = 'Data (%d points)' % (u_train.shape[0]), \n",
    "    markersize = 4,  # marker size doubled\n",
    "    clip_on = False,\n",
    "    alpha=1.0\n",
    ")\n",
    "\n",
    "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "\n",
    "ax.set_xlabel('$t$', size=20)\n",
    "ax.set_ylabel('$x$', size=20)\n",
    "ax.legend(\n",
    "    loc='upper center', \n",
    "    bbox_to_anchor=(0.9, -0.05), \n",
    "    ncol=5, \n",
    "    frameon=False, \n",
    "    prop={'size': 15}\n",
    ")\n",
    "ax.set_title('$u(t,x)$', fontsize = 20) # font size doubled\n",
    "ax.tick_params(labelsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Row 1: u(t,x) slices ################## \n",
    "\n",
    "\"\"\" The aesthetic setting has changed. \"\"\"\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "gs1 = gridspec.GridSpec(1, 3)\n",
    "gs1.update(top=1-1.0/3.0-0.1, bottom=1.0-2.0/3.0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 0])\n",
    "ax.plot(x,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')    \n",
    "ax.set_title('$t = 0.25$', fontsize = 15)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 1])\n",
    "ax.plot(x,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "ax.set_title('$t = 0.50$', fontsize = 15)\n",
    "ax.legend(\n",
    "    loc='upper center', \n",
    "    bbox_to_anchor=(0.5, -0.15), \n",
    "    ncol=5, \n",
    "    frameon=False, \n",
    "    prop={'size': 15}\n",
    ")\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 2])\n",
    "ax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])    \n",
    "ax.set_title('$t = 0.75$', fontsize = 15)\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python37564bitpytorchcondaef97e2e14a3440b09e2af926cca799cf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
